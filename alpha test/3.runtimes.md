
### **第零章：专家工作坊 — 节点内部的协作之舞**

我们已经知道，“图”是您设计的创作蓝图，“节点”是您剧组里各司其职的专家。但如果一个专家的任务本身就比较复杂，需要分好几个步骤才能完成呢？比如，您要求一位“书记员”专家，不仅要“记录”一段对话，还要紧接着对它进行“总结”。

这就引出了一个核心问题：**在一个节点（一位专家）的内部，多个连续的动作是如何衔接的？数据又是如何在这些动作之间传递的？**

欢迎来到专家的“工作坊”。在这里，我们将揭示 Hevno 最强大的机制之一：节点内的**指令管道 (Instruction Pipeline)**。

#### **1. 指令列表 (`run`)：专家的“行动清单”**

在 Hevno 的编辑器中，当您配置一个节点时，核心区域是一个名为 `run` 的列表。您可以把它想象成是您为这位专家（节点）亲手编写的 **“行动清单”** 或 **“工作步骤表”**。

*   **严格的顺序**: 这个清单上的每一项（我们称之为**指令 (Instruction)**）都会被**严格按照从上到下的顺序**依次执行。第一步完成之前，绝不会开始第二步。
*   **独立的技能**: 每一条指令，您都可以为其装备一个不同的“运行时”（技能）。这意味着，您的“书记员”专家可以在第一步使用 `memoria.add` 技能来“记录”，在第二步使用 `llm.default` 技能来“总结”。

这个有序的“行动清单”，是您精确控制专家工作流程的基础。

#### **2. 管道 (`pipe`)：专家的私人“工作台”**

现在，最关键的问题来了：当第一步指令（记录对话）完成后，第二步指令（总结对话）如何拿到刚刚被记录的、热乎乎的对话内容呢？

答案就是 **`pipe`** 对象。

请将 `pipe` 想象成这位专家面前的一张**私人的、临时的“工作台”**。

*   **它的生命周期**: 这张“工作台”在专家（节点）开始工作时是**完全干净的**。工作全部结束后，工作台上的所有东西都会被**立刻清空**。它是一个只存在于单个节点执行期间的临时存储空间。
*   **它的核心规则**: 每当“行动清单”上的一条指令执行完毕后，它会把自己产生的结果**自动放到**这张工作台 (`pipe`) 上。如果工作台上已经有东西了，新的结果会**覆盖**旧的。

`pipe` 是连接一个节点内部所有指令的生命线。它是实现多步骤、复杂任务处理的核心机制。

#### **3. 宏 `{{ ... }}`：从“工作台”取用工具的密语**

我们有了“工作台” (`pipe`)，但如何告诉下一条指令“去工作台上拿你需要的数据”呢？这就需要用到 Hevno 的“密语”——**宏 (Macro)**。

在 Hevno 的编辑器中，您会看到许多参数输入框。在这些输入框里，您随时可以使用双大括号 `{{ ... }}` 来插入一段“密语”。

*   **它的作用**: 宏就像一个**动态的“便利贴”**。当引擎准备执行一条指令时，它会先检查这条指令的所有参数。一旦发现 `{{ ... }}` 这样的便利贴，它会立刻根据便利贴上的内容，去寻找对应的数据，然后用找到的数据**替换**掉这张便利贴。
*   **最常用的密语**: `{{ pipe.output }}`。这句密语的意思非常直白：“**去 `pipe`（工作台）上，找到名为 `output` 的那个结果。**”

通过这个简单的“密语”系统，您就可以轻松地让一条指令，使用它前面所有指令留下来的成果。

#### **4. 完整流程演示：一位“分析师”的工作日**

让我们通过一个完整的例子，将以上所有概念串联起来。

**您的目标**: 创建一个“推特分析师”节点。它的任务是：1. 接收一条推文内容；2. 调用 AI 判断这条推文的情感是积极还是消极。

在 Hevno 编辑器中，您将这样设计这位“分析师”的“行动清单” (`run` 列表)：

---

**第一步：接收输入**

*   **您为它装备的技能 (运行时)**: `system.io.input`
    *   这个技能非常简单，它的唯一作用就是接收一个值，然后把它放到“工作台” (`pipe`) 上。
*   **您为它配置的参数**:
    *   `value`: 在这个参数的输入框里，您输入了 `{{ run.triggering_input.tweet_content }}`。
        *   *(这是一个宏，它会从触发这次图运行的外部输入中，找到名为 `tweet_content` 的数据。现在您只需知道，它能帮我们拿到推文内容即可。)*

*   **幕后发生的事**:
    1.  引擎执行第一条指令。
    2.  `system.io.input` 技能启动，拿到了推文内容，比如是：“今天阳光真好，我爱这个世界！”
    3.  它将这个字符串，以 `output` 的名字，**放到了“工作台” (`pipe`) 上**。
    4.  **此刻，“工作台”的状态是**：`{ "output": "今天阳光真好，我爱这个世界！" }`

---

**第二步：分析情感**

*   **您为它装备的技能 (运行时)**: `llm.default`
    *   这个技能的专长是与大语言模型对话。
*   **您为它配置的参数**:
    *   `model`: 您选择了一个轻快、高效的模型，如 `gemini/gemini-1.5-flash`。
    *   `contents` (或 `prompt`): 在这个参数的输入框里，您精心构建了一个指令，但关键部分使用了我们的“密语”：
        ```
        请判断以下文本的情感是“积极”还是“消极”，只返回一个词：
        {{ pipe.output }}
        ```

*   **幕后发生的事**:
    1.  引擎开始执行第二条指令。
    2.  在真正调用 AI 之前，引擎看到了便利贴 `{{ pipe.output }}`。
    3.  它立刻**查看“工作台”**，找到了 `output` 对应的值：“今天阳光真好，我爱这个世界！”
    4.  它用这个值**替换**了便利贴。最终，发送给 AI 的完整指令是：
        ```
        请判断以下文本的情感是“积极”还是“消极”，只返回一个词：
        今天阳光真好，我爱这个世界！
        ```
    5.  AI 完成任务，返回了结果：“积极”。
    6.  `llm.default` 技能接收到这个结果，然后**也以 `output` 的名字，把它放到了“工作台”上**。
    7.  **此刻，“工作台”的状态被更新为**：`{ "output": "积极" }`

---

**工作完成！**

这位“分析师”节点的所有指令都执行完毕了。它最终的成果——“积极”——现在就存放在它的最终输出中，可以被图中的其他节点使用了。而那张临时的“工作台” (`pipe`)，则被立刻清空，等待下一次任务。

**总结一下，您作为“世界总设计师”需要记住的核心协作机制：**

> 1.  **节点内的指令 (`run` 列表) 是一个有序的、一步接一步的行动清单。**
> 2.  **`pipe` 是节点内部的临时“工作台”，用于在这些步骤之间传递数据。**
> 3.  **宏 `{{ ... }}` 是您指挥指令“去哪里拿数据”的“密语”。**

掌握了这个机制，您就拥有了创造出功能极其强大、逻辑极其复杂的“AI 专家”的能力，而这一切，都无需编写一行代码。接下来，我们将正式进入“技能库”，详细了解每一个运行时（技能）的具体用法。



### **第一章：`llm.default` — 创作的灵魂**

#### **一句话摘要**

该运行时是您与大语言模型（LLM）沟通的核心桥梁，负责将精心准备的创作指令发送给 AI，并获取其生成的内容。

#### **为何存在？(它的“专家角色”)**

在您的“AI 专家剧组”中，`llm.default` 运行时所扮演的，无疑是那位万众瞩目的 **“首席作家”** 或 **“灵感演绎者”**。

它的天职不是处理琐碎的杂务。它不负责从冗长的历史中筛选记忆，也不负责解析复杂的数据。它的使命是**纯粹的、专注的、高质量的文本创作**。

`llm.default` 的强大之处，在于它将“准备素材”和“挥洒文采”这两个阶段彻底分离。您剧组里的其他专家（节点）会将所有必需的背景、记忆、规则都准备妥当，然后将一份**干净、高效、结构清晰的“创作大纲”** 递交给它。

收到这份大纲后，“首席作家”便可以心无旁骛，将全部的“心智”都投入到文学创作中，为您生成真正动人、深刻、且符合您期望的精彩内容。

#### **配置参数 (Configuration)**

当您为一个指令装备上 `llm.default` 这个“技能”时，您需要配置以下参数来指导它的工作：

*   **`model`** (`文本`, **必需**)
    *   您希望这位“首席作家”具体使用哪个大脑进行创作。您需要提供一个完整的模型名称，例如 `gemini/gemini-1.5-pro` 或 `gemini/gemini-1.5-flash`。选择更强大的模型通常会带来更高的创作质量，但也意味着更高的成本和更长的等待时间。

*   **`contents`** (`列表`, **必需**)
    *   **这是 `llm.default` 的核心，是您递交给“首席作家”的最终“创作大纲”或“剧本”**。它不是一段简单的文本，而是一个有序的、结构化的列表。列表中的每一项都是一个独立的“指令块”，让您可以极其精确地构建与 AI 的对话。
    *   在编辑器中，您可以向这个列表里添加两种不同类型的“指令块”：

        *   **类型一：`MESSAGE_PART` (消息片段)**
            *   这是最基础的指令块，代表一次完整的对话发言。它包含：
                *   `role` (`文本`): 这段话是谁说的？通常是 `system` (系统指令)、`user` (您的指令) 或 `model` (AI 之前的回复)。
                *   `content` (`任意类型`): 这段话的具体内容是什么。这里**完美支持宏**，您可以将任何需要动态填入的数据，用 `{{ ... }}` 的形式放在这里。

        *   **类型二：`INJECT_MESSAGES` (历史注入)**
            *   这是一个极其强大的指令块，专门用于解决“长对话记忆”的问题。它能将您从其他节点（例如“记忆检索师”）获取到的、已经整理好的对话历史记录，**无缝地、原封不动地**插入到当前对话的这个位置。它包含：
                *   `source` (`任意类型`): 您需要告诉它去哪里获取这段历史记录。这里**必须使用宏**，例如 `{{ nodes.get_chat_history.output }}`，意思是“去名为 `get_chat_history` 的那个节点，把它最终的产出 (`output`) 拿过来”。

    *   **通用控制参数**:
        *   `is_enabled` (`布尔值`, 可选): 每一个“指令块”都有这个开关。您可以**使用宏** `{{ ... }}` 来动态决定这个指令块在本次执行中是否要被激活。例如，您可以设置 `{{ len(nodes.get_chat_history.output) > 0 }}`，意思是“只有当聊天记录不为空时，才注入历史记录”。

*   **其他高级参数** (可选)
    *   除了以上核心参数，您还可以直接添加如 `temperature`, `top_p`, `max_tokens` 等高级参数。这些参数会被直接传递给底层的 LLM API，让您能像专家一样，对 AI 的生成风格进行更精细的微调。

#### **输出 (Output)**

当“首席作家”完成工作后，它会把它本次的成果，以一个结构化对象的形式，放到当前节点的“工作台” (`pipe`) 上。您可以通过 `{{ pipe.output }}` 来获取它返回的文本内容。完整的输出包含：

*   `output` (`文本`): AI 生成的核心文本内容。
*   `usage` (`对象`): 本次调用消耗的 token 数量统计，便于您进行成本控制。
*   `model_name` (`文本`): 本次调用实际使用的模型名称。
*   `error` (`文本`, 仅在失败时出现): 如果调用失败，这里会包含详细的错误信息。

#### **核心示例 (In a Nutshell)**

让我们用一个最简单的例子，来感受一下 `llm.default` 的基本用法。

**目标**: 创建一个“笑话大师”节点，让它讲一个关于程序员的笑话。

1.  在您的图中，创建一个新的节点，命名为 `tell_a_joke`。
2.  在它的“行动清单” (`run`) 中，添加一条指令，并为这条指令装备上 `llm.default` 技能。
3.  **配置参数**:
    *   在 `model` 输入框中，填入 `gemini/gemini-1.5-flash`。
    *   在 `contents` 列表中，添加一个 `MESSAGE_PART` 类型的指令块：
        *   将其 `role` 设置为 `user`。
        *   将其 `content` 设置为 `请讲一个关于程序员的简短笑话。`

当这个节点运行时，它就会调用 Gemini Flash 模型，获取一个笑话，并将其作为节点的最终输出。

#### **协同工作 (Synergy in Practice)：构建拥有记忆的聊天机器人**

现在，让我们见证真正的魔法。我们将展示 `llm.default` 如何与剧组中的其他专家协同，优雅地构建一个能够联系上下文的聊天机器人，彻底告别手动拼接历史记录的痛苦。

**您的蓝图 (图) 设计**:

您需要两位专家（两个节点）来完成这个任务：

1.  **节点 A: `get_chat_history` (记忆检索师)**
    *   **它的任务**: 它的唯一职责，就是从世界的记忆中，把最近的 10 条对话历史给找出来。
    *   **它装备的技能**: `memoria.query` (我们将在后续章节详细介绍这个技能)。
    *   **它的输出**: 一个格式完美的、包含过去对话的列表。

2.  **节点 B: `generate_response` (首席作家)**
    *   **它的任务**: 接收“记忆检索师”整理好的历史，结合用户的当前输入，生成一句连贯的回应。
    *   **它的工作前提**: **必须等待** `get_chat_history` 节点完成工作后，才能开始。
    *   **它装备的技能**: `llm.default`。

**“首席作家” (`generate_response` 节点) 的参数配置**:

*   **`model`**: `gemini/gemini-1.5-pro` (我们希望它有最好的对话能力)。

*   **`contents` (创作大纲)**: 您将在这个列表里，按照顺序，精心排布三个指令块：

    1.  **第一块: 设定人设 (System Prompt)**
        *   **类型**: `MESSAGE_PART`
        *   **`role`**: `system`
        *   **`content`**: `你是一个乐于助人的 AI 助手，名叫 Hevno。`

    2.  **第二块: 注入历史 (Inject History)**
        *   **类型**: `INJECT_MESSAGES`
        *   **`source`**: `{{ nodes.get_chat_history.output }}`
            *   *(这句“密语”告诉引擎：去 `get_chat_history` 节点，把它产出的那个列表拿过来，原封不动地插在这里。)*
        *   **`is_enabled`**: `{{ len(nodes.get_chat_history.output) > 0 }}`
            *   *(这句“密语”是一个安全开关：只有当历史记录确实存在时，才执行注入操作。)*

    3.  **第三块: 用户当前输入 (Current Input)**
        *   **类型**: `MESSAGE_PART`
        *   **`role`**: `user`
        *   **`content`**: `{{ run.triggering_input.user_message }}`
            *   *(这句“密语”告诉引擎：去获取触发这次运行的外部输入，找到那条用户发来的最新消息。)*

**最终效果**:

当这个图运行时，Hevno 引擎会自动为您完成所有繁重的工作。它会先让“记忆检索师”取回历史，然后将这三块内容——**系统人设、对话历史、当前问题**——完美地、按顺序地组装成一个结构化的请求，最后才交给“首席作家” `llm.default`。

“首席作家”收到的，是一份无懈可击的、包含了所有必要上下文的剧本。它因此能够轻松地理解对话的来龙去脉，并给出一个无比连贯和智能的回答。

您，作为“世界总设计师”，仅仅是通过这样直观地排布几个指令块，就构建出了一个过去需要大量复杂代码才能实现的、健壮而强大的对话系统。这，就是 `llm.default` 与 Hevno 流图协作的真正力量。


### **第二章：`system` (IO & Data) — 剧组的幕后英雄**

本章节将介绍 Hevno 引擎中负责基础数据流转和处理的核心工具集。它们是构建任何复杂工作流都必不可少的“基础设施”，是您剧组里那些确保一切顺利运行的“幕后英雄”。

### **2.1 `system.io.input`: 数据的官方入口**

#### **一句话摘要**

将一个您指定的值，作为后续步骤的起始数据，放入当前节点的“工作台” (`pipe`) 中。

#### **为何存在？(它的“专家角色”)**

在您的剧组里，`system.io.input` 扮演着 **“场记板”** 或 **“数据接收员”** 的角色。

它的职责极其单纯和重要：为节点内部的一系列复杂操作，**提供一个清晰、统一的起点**。当您需要从外部世界（比如用户的输入、或者上一个节点的计算结果）获取一个数据，并以此为基础展开一连串的加工处理时，`system.io.input` 就是您打下的第一块基石。

它会接过您递来的数据，郑重地把它放在节点的“工作台”上，并命名为 `output`，然后大喊一声：“开拍！”，后续的所有指令便可以围绕这个初始数据开始工作了。

#### **配置参数 (Configuration)**

*   **`value`** (`任意类型`, **必需**)
    *   您希望放入“工作台”的那个具体的值。这里**完美支持宏**，让您可以从任何地方动态地获取数据。例如：
        *   `{{ run.triggering_input.damage }}`: 从触发本次运行的外部输入中，获取 `damage` 的值。
        *   `{{ nodes.another_node.output.result }}`: 从另一个已完成的节点 `another_node` 的输出结果中，获取 `result` 字段的值。
        *   `一个静态的字符串`: 您也可以直接输入一个固定的值。

#### **输出 (Output)**

执行完毕后，它会向节点的“工作台” (`pipe`) 输出一个非常简单的结构：

*   `output`: 其值就是您在 `value` 参数中提供的那个值。

#### **核心示例 (In a Nutshell)**

**目标**: 创建一个节点，其任务是接收一个伤害数值，为后续的计算做准备。

1.  在您的图中，创建一个新节点，命名为 `prepare_damage_calculation`。
2.  在它的“行动清单” (`run`) 中，添加第一条指令，并装备上 `system.io.input` 技能。
3.  **配置参数**:
    *   在 `value` 输入框中，填入宏 `{{ run.triggering_input.damage_amount }}`。

当这个节点的指令开始执行时，它会立刻获取到外部传入的伤害值，并将其置于“工作台”上，等待后续指令（比如一个 `system.execute` 指令）来取用 `{{ pipe.output }}` 并更新玩家的生命值。

### **2.2 `system.data.format`: 文本的排版师**

#### **一句话摘要**

将一个列表（比如多条记忆）或一个字典，按照您设计的模板，优雅地格式化成一段单一、连贯的文本。

#### **为何存在？(它的“专家角色”)**

如果说 `llm.default` 是“首席作家”，那么 `system.data.format` 就是他身边那位一丝不苟的 **“文本排版师”** 或 **“资料整理助手”**。

“首席作家”（LLM）虽然才华横溢，但他们更喜欢阅读结构清晰、格式优美的自然语言，而不是原始、杂乱的数据结构。直接将一个程序化的列表（例如，从 `memoria.query` 拿到的记忆条目数组）丢给 LLM，效果往往不佳，甚至会造成它的困惑。

“文本排版师”的职责，就是在这两者之间架起一座桥梁。它会接过那些结构化的数据，根据您提供的“排版规则”（模板），将它们整理成一段精心编排、可读性极强的文本。这份经过精美排版的“资料汇编”，能让“首席作家”在接收时一目了然，从而极大地提升其创作质量和效率。

#### **配置参数 (Configuration)**

*   **`items`** (`列表` 或 `字典`, **必需**)
    *   您需要进行排版的原始数据源。通常，这里会使用宏来引用前序节点的结果，例如 `{{ nodes.get_recent_memories.output }}`。

*   **`template`** (`文本`, **必需**)
    *   您的“排版规则”。这是一个模板字符串，您可以在其中使用特殊的占位符来告诉排版师如何处理每一项数据。
        *   如果 `items` 是一个**列表**，您可以使用 `{item}` 占位符来代表列表中的每一项。如果列表中的项是对象，您还可以使用点或方括号访问其内部属性，例如 `{item.content}` 或 `{item[source]}`。
        *   如果 `items` 是一个**字典**，您可以使用 `{key}` 和 `{value}` 占位符。

*   **`joiner`** (`文本`, 可选)
    *   当所有项都根据模板格式化完毕后，用什么字符串将它们连接起来。默认是换行符 `\n`，这意味着每一项都会另起一行。

#### **输出 (Output)**

执行完毕后，它会向节点的“工作台” (`pipe`) 输出：

*   `output` (`文本`): 经过排版和连接后的最终成品字符串。

#### **协同工作 (Synergy in Practice): 为作家整理记忆摘要**

**目标**: 从记忆库中检索最近的 3 个事件，并将它们格式化为一份清晰的“前情提要”，供“首席作家”参考。

**您的蓝图 (图) 设计**:

1.  **节点 A: `retrieve_events` (记忆检索师)**
    *   **技能**: `memoria.query`
    *   **任务**: 从记忆库中查询最新的 3 个“事件”级别的记忆。
    *   **输出**: 一个包含三个记忆对象的列表，每个对象都有 `content` 和 `source` 字段。

2.  **节点 B: `format_summary` (文本排版师)**
    *   **技能**: `system.data.format`
    *   **工作前提**: 必须等待 `retrieve_events` 完成。
    *   **配置参数**:
        *   **`items`**: `{{ nodes.retrieve_events.output }}` (引用“记忆检索师”的成果)
        *   **`template`**: `- 事件回顾: {item.content} (来源: {item.source})` (定义排版规则)
        *   **`joiner`**: `\n` (每个事件占一行)

**最终效果**:

假设“记忆检索师”返回了三条记忆。经过“文本排版师” `system.data.format` 的精心处理，传递给下一个 LLM 节点的，将不再是杂乱的程序对象，而是一段极其清晰优美的文本：

```
- 事件回顾: 玩家在森林入口遭遇并击败了三只哥布林。 (来源: 战斗系统)
- 事件回顾: 村长请求玩家帮忙寻找丢失的宝物。 (来源: 对话系统)
- 事件回顾: 玩家在商店购买了一瓶治疗药水。 (来源: 交易系统)
```

这份完美的“前情提要”，能让您的“首席作家”立刻进入状态，写出与上下文无缝衔接的精彩剧情。

### **2.3 `system.data.parse`: 格式的解码员**

#### **一句话摘要**

将一段包含特定格式（如 JSON）的文本字符串，“解码”回系统可以理解的结构化数据（如对象或字典）。

#### **为何存在？(它的“专家角色”)**

在我们的剧组中，`system.data.parse` 扮演着 **“格式解码员”** 或 **“情报分析员”** 的角色。

有时候，我们会要求“首席作家”（LLM）在创作的同时，返回一些结构化的数据。例如，要求它在写完一段内心独白后，以 JSON 格式返回角色当前的情绪状态：`{"emotion": "wary", "intensity": 8}`。

然而，LLM 返回的这个结果本质上仍然是一段**纯文本**。系统无法直接理解“emotion”是什么意思。这时，“格式解码员”就派上了用场。它的任务就是接收这段看起来像代码的文本，并将其**真正地转换**成系统内部可以访问和操作的结构化数据。

通过它，我们得以让 AI 的输出超越单纯的文字，实现“创作”与“数据”的双重产出。

#### **配置参数 (Configuration)**

*   **`text`** (`文本`, **必需**)
    *   需要被解码的原始文本字符串。通常，这里会使用宏 `{{ pipe.output }}` 或 `{{ nodes.llm_node.output }}` 来引用上一步 LLM 的输出。

*   **`format`** (`文本`, **必需**)
    *   您期望解码员以哪种“语言规范”来解读这段文本。目前支持：
        *   `"json"`: 这是最常用的格式，用于解析 JSON 字符串。
        *   `"xml"`: 用于解析 XML 格式的文本。

*   **`strict`** (`布尔值`, 可选)
    *   是否启用“严格模式”，默认为 `false`。
        *   在**非严格模式**下，如果文本格式有误（例如，一个不完整的 JSON），解码员不会中断整个流程，而是会返回一个包含错误信息的对象，让您可以进行后续的容错处理。
        *   在**严格模式**下，一旦解码失败，它会直接报错，并中止当前节点的执行。

#### **输出 (Output)**

*   **成功时**: `output` (`对象` 或 `字典`): 解码后的结构化数据。
*   **失败时 (非严格模式)**: `output` (`对象`): 一个包含 `error` 字段的对象，其中记录了失败的原因。

#### **协同工作 (Synergy in Practice): 让 AI “思考”并输出结构化决策**

**目标**: 我们希望 AI 在决定 NPC 下一步行动时，不仅生成对话，还能以 JSON 格式输出其背后的“思考过程”和最终“决策”。

**您的蓝图 (图) 设计**:

1.  **节点 A: `npc_thinks` (首席作家)**
    *   **技能**: `llm.default`
    *   **任务**: 指导 AI 同时完成两件事：生成对话，并以 JSON 格式输出思考过程。发送给它的指令可能如下：
        ```
        你扮演一个警惕的守卫。你看到玩家走近。请生成你的内心思考和行动决策，并严格按照以下 JSON 格式包裹你的输出：
        {"thought": "这人看起来不像坏人，但我还是得按规矩盘问。", "action": "AskForIdentification", "dialogue": "站住！你是什么人？"}
        ```    *   **输出**: 一段包含上述 JSON 的纯文本字符串。

2.  **节点 B: `decode_decision` (格式解码员)**
    *   **行动清单 (`run`)**:
        1.  **第一步: 接收 AI 的原始文本**
            *   **技能**: `system.io.input`
            *   **参数 `value`**: `{{ nodes.npc_thinks.output }}`
        2.  **第二步: 解码 JSON**
            *   **技能**: `system.data.parse`
            *   **参数 `text`**: `{{ pipe.output }}` (引用上一步接收到的文本)
            *   **参数 `format`**: `"json"`

**最终效果**:

“首席作家” `npc_thinks` 完成后，它的输出是一段纯文本。但紧接着，“格式解码员” `decode_decision` 会接过这段文本，并将其成功“解码”。

`decode_decision` 节点的最终输出，将不再是文本，而是一个**真正的、结构化的对象**：

```json
{
  "output": {
    "thought": "这人看起来不像坏人，但我还是得按规矩盘问。",
    "action": "AskForIdentification",
    "dialogue": "站住！你是什么人？"
  }
}
```

现在，图中的其他节点就可以轻松地使用宏 `{{ nodes.decode_decision.output.action }}` 来获取“`AskForIdentification`”这个决策，并据此触发后续的游戏逻辑。这完美地展示了如何让 AI 的能力从“只会说话”升级到“能思考、能决策”。

### **2.4 `system.data.regex`: 文本的手术刀**

#### **一句话摘要**

使用正则表达式（Regex），从一段文本中精确地、像外科手术一样提取出您需要的部分。

#### **为何存在？(它的“专家角色”)**

在我们的剧组中，`system.data.regex` 扮演着 **“文本外科医生”** 或 **“情报提取专家”** 的角色。

有时候，`system.data.parse` 这样的“解码员”会遇到一些更棘手的情况。比如，AI 可能没有严格遵循您要求的 JSON 格式，而是在一长段的自然语言描述中，将关键信息用特殊的标签（如 `<thinking>...</thinking>`）包裹了起来。

这时，我们就需要一位技艺更高超的专家。`system.data.regex` 就是这样一位专家，它手持名为“正则表达式”的精密“手术刀”，能够无视周围的无关文本，直奔主题，精准地切出并提取被特定模式包裹起来的核心内容。

当您需要从非结构化或半结构化的文本中，进行最精细、最可靠的内容提取时，这位“文本外科医生”是您的不二之-选。

#### **配置参数 (Configuration)**

*   **`text`** (`文本`, **必需**)
    *   需要进行手术的原始文本。通常使用宏 `{{ pipe.output }}` 或 `{{ nodes.some_node.output }}`。

*   **`pattern`** (`文本`, **必需**)
    *   您的“手术方案”——一个正则表达式字符串。这是您告诉“医生”要寻找和提取什么模式的指令。您可以在模式中使用“命名捕获组” (`?P<name>...`) 来为您想要提取的部分命名。

*   **`mode`** (`文本`, 可选)
    *   手术的模式，默认为 `"search"`。
        *   `"search"`: 在整个文本中寻找**第一个**匹配项。如果您的模式里有命名捕获组，它会返回一个包含这些捕获组内容的对象；如果没有，则返回整个匹配到的字符串。
        *   `"find_all"`: 在整个文本中寻找**所有**匹配项，并将它们以列表的形式返回。

#### **输出 (Output)**

*   `output`: 提取到的结果。其具体类型取决于您的 `pattern` 和 `mode`。
    *   如果 `mode` 是 `"search"` 且有命名捕获组，它是一个包含这些组的对象。
    *   如果 `mode` 是 `"search"` 且无命名捕获组，它是一个字符串。
    *   如果 `mode` 是 `"find_all"`，它是一个列表。
    *   如果找不到任何匹配项，它将是 `null` (空值)。

#### **核心示例 (In a Nutshell)**

**目标**: AI 返回了一段混杂的文本：`"经过一番思考 <thinking>我应该先警告他</thinking>，守卫决定拔出剑。"`, 我们需要精确地提取出 `<thinking>` 标签里的内容。

1.  创建一个节点 `extract_thought`。
2.  在它的 `run` 列表中添加一条指令，装备 `system.data.regex` 技能。
3.  **配置参数**:
    *   **`text`**: (假设 AI 的输出在 `pipe.output` 中) `{{ pipe.output }}`
    *   **`pattern`**: `<thinking>(?P<thought>.+?)</thinking>`
        *   *(这个复杂的“手术方案”意思是：寻找 `<thinking>` 和 `</thinking>` 标签，并把它们中间的所有内容，捕获并命名为 `thought`。)*
    *   **`mode`**: `"search"`

**最终效果**:

该节点执行后，其输出将会是一个结构清晰的对象：

```json
{
  "output": {
    "thought": "我应该先警告他"
  }
}
```

通过这种方式，您即使在面对不完全合作、格式不完美的 AI 时，依然有强大的后备手段来保证关键信息的稳定提取，极大地增强了您整个系统的健壮性。


### **第三章：`memoria` — 赋予世界记忆**

本章节将深入探讨 Hevno 引擎的记忆系统。`memoria` 运行时家族是您赋予 AI 角色和整个世界“短期记忆”和“长期历史感”的核心工具。它们扮演着世界的 **“书记员”** 和 **“历史学家”** 的双重角色，确保您所创造的世界是一个有过去、有未来的动态实体。

### **3.1 `memoria.add`: 烙印新记忆**

#### **一句话摘要**

将一个事件、一段对话或一个想法，作为一个结构化的“记忆条目”，永久地记录到指定的“记忆回廊”中。

#### **为何存在？(它的“专家角色”)**

在您的剧组中，`memoria.add` 扮演着那位手持羽毛笔、一丝不苟的 **“书记员”**。

它的职责神圣而明确：**确保任何有价值的信息都不会被遗忘**。无论是玩家的一句关键对话、一次惊心动魄的战斗结果，还是 NPC 的一次内心顿悟，这位“书记员”都会立刻将其转化为一条标准的、结构化的档案记录，并存入世界的“记忆宫殿”中。

它记录的不仅仅是文本，更是包含了时间、层级和标签的“富文本”记忆，为日后的精准检索奠定了坚实的基础。每一次 `memoria.add` 的调用，都是在为您的世界增添一份厚重的历史底蕴。

#### **配置参数 (Configuration)**

*   **`stream`** (`文本`, **必需**)
    *   您希望将这条新记忆存放到哪个“记忆回廊”（Memory Stream）里。您可以把它想象成是档案馆里的不同“卷宗”。例如，`"chat_history"` 用于存放对话历史，`"main_quest_log"` 用于记录主线任务进展，`"character_thoughts_npc1"` 用于记录某个 NPC 的心路历程。

*   **`content`** (`任意类型`, **必需**)
    *   记忆的具体内容。这里**完美支持宏**，您可以将任何需要记录的动态信息传入。最终，无论传入何种类型，它都会被转换为文本进行存储。

*   **`level`** (`文本`, 可选)
    *   为这条记忆设定一个“层级”，默认为 `"event"`。这就像是为档案打上“普通”、“重要”、“绝密”的标签。您可以自定义任何层级，如 `"dialogue"` (对话), `"milestone"` (里程碑), `"thought"` (想法)。
    *   **重要约定**: 当您记录的是一段对话时，强烈推荐将该角色的身份（如 `"user"` 或 `"model"`）作为 `level` 的值。这为后续 `memoria.query` 将其格式化为标准的对话历史列表提供了关键信息。

*   **`tags`** (`列表`, 可选)
    *   为这条记忆贴上一个或多个“关键词标签”。这是一个极其强大的功能，允许您进行跨“记忆回廊”的主题式检索。例如，您可以为所有与“战斗”相关的记忆，无论它发生在哪个任务线中，都打上 `["combat"]` 的标签。

#### **输出 (Output)**

执行成功后，它会向节点的“工作台” (`pipe`) 输出一个对象，其中包含了刚刚被创建并存入数据库的那个完整的 `MemoryEntry`（记忆条目）对象。这在您需要立即获取新记忆的 ID 或其他元数据时非常有用。

*   `output` (`对象`): 包含 `id`, `sequence_id`, `content`, `level`, `tags` 等字段的记忆条目对象。

#### **核心示例 (In a Nutshell)**

**目标**: 创建一个“对话记录员”节点，将用户的最新发言记录到聊天历史中。

1.  在您的图中，创建一个新节点，命名为 `record_user_chat`。
2.  在它的“行动清单” (`run`) 中，添加一条指令，并装备上 `memoria.add` 技能。
3.  **配置参数**:
    *   **`stream`**: `"chat_history"`
    *   **`content`**: `{{ run.triggering_input.user_message }}` (使用宏获取外部传入的用户消息)
    *   **`level`**: `"user"` (遵循约定，标记这是用户的发言)
    *   **`tags`**: `["dialogue"]` (为这条记忆打上“对话”的标签)

当这个节点运行时，用户的发言就会被转化为一条结构化的记忆，被永久地烙印在 `chat_history` 这条记忆回廊里。

### **3.2 `memoria.query`: 检索历史片段**

#### **一句话摘要**

根据您提供的多种条件（如数量、层级、标签），从指定的“记忆回廊”中精准地筛选和检索出符合要求的历史记忆片段。

#### **为何存在？(它的“专家角色”)**

如果说 `memoria.add` 是“书记员”，那么 `memoria.query` 就是您剧组里那位戴着单片眼镜、博闻强识的 **“历史学家”**。

“首席作家”（LLM）在创作时，最需要的就是精准、相关的上下文。将整个档案馆的所有资料都丢给它，只会让它不堪重负。这位“历史学家”的职责，就是根据“首席作家”当前的需求，深入记忆宫殿，快速地找出最相关的那几页历史档案。

它能够**按时间检索**（“最近发生了什么？”）、**按层级检索**（“把所有重要的里程碑事件都找出来”）、**按主题检索**（“找出所有与‘背叛’这个主题相关的记忆”），并将这些宝贵的历史片段，以干净整洁的格式，呈递给“首席作家”。

#### **配置参数 (Configuration)**

*   **`stream`** (`文本`, **必需**)
    *   您希望从哪个“记忆回廊”中进行查询。

*   **`latest`** (`数字`, 可选)
    *   仅返回最新的 N 条记忆。这是实现“短期记忆”最常用的参数。例如，设置为 `10` 意味着只看最近的 10 次交互。

*   **`levels`** (`列表`, 可选)
    *   一个包含层级名称的列表，它会只返回 `level` 字段匹配列表中任意一项的记忆。例如，`["user", "model"]` 会只返回所有对话内容。

*   **`tags`** (`列表`, 可选)
    *   一个包含标签名称的列表。它会返回**至少包含其中一个标签**的所有记忆。

*   **`order`** (`文本`, 可选)
    *   返回结果的排序方式，基于记忆的发生顺序。可以是 `"ascending"`（升序，默认）或 `"descending"`（降序）。

*   **`format`** (`文本`, 可选)
    *   定义输出结果的格式，默认为 `"raw_entries"`。
        *   `"raw_entries"`: 返回完整的记忆条目对象列表，包含所有元数据。
        *   `"message_list"`: **一个专为 `llm.default` 设计的强大模式**。它会自动将 `level` 为 `"user"` 或 `"model"` 的记忆条目，转换成 `llm.default` 的 `INJECT_MESSAGES` 指令块最喜欢的 `[{"role": "user", "content": "..."}, ...]` 格式。

#### **输出 (Output)**

*   `output` (`列表`): 一个包含所有符合查询条件的记忆条目的列表。列表内元素的格式，由您配置的 `format` 参数决定。

#### **协同工作 (Synergy in Practice)**

`memoria.add` 和 `memoria.query` 天生就是一对黄金搭档。它们构成了驱动您的 AI 世界进行持续、连贯互动的基础引擎。

在我们在 `llm.default` 章节中已经演示过的“**构建拥有记忆的聊天机器人**”的例子中：

1.  图的开始，一个 `memoria.add` 节点扮演**“书记员”**，将用户的当前输入记录下来。
2.  紧接着，一个 `memoria.query` 节点扮演**“历史学家”**，使用 `latest: 10` 和 `format: "message_list"` 参数，干净利落地取回了最近的对话历史，并直接将其整理成了 LLM 最喜欢的格式。
3.  最后，`llm.default` 节点作为**“首席作家”**，通过 `INJECT_MESSAGES` 指令块，轻松地接收了这份由“历史学家”精心准备的“前情提要”，从而生成了与上下文完美契合的回答。

这个“记录 -> 检索 -> 创作”的闭环工作流，是您在 Hevno 中构建几乎所有动态交互的基础模式，也是 `memoria` 运行时家族的核心价值所在。


### **第四章：`codex` — 定义世界的知识**

本章节将为您介绍 Hevno 引擎的动态知识库系统。`codex.invoke` 运行时是您剧组中的 **“规则大师”** 与 **“世界观设定师”**。它不关心线性发展的历史事件，而是负责根据当前情境，从您预设的庞大“知识法典”中，智能地抽取、组合并渲染出最恰当的知识片段，为“首席作家”提供逻辑严密、细节丰富的创作基石。

### **`codex.invoke`: 知识的动态组合器**

#### **一句话摘要**

根据您提供的触发条件（如关键词），从 `lore`（世界法典）和 `moment`（即时快照）中收集相关的“知识条目”，并将它们按优先级智能地组合成一段连贯的文本。

#### **为何存在？(它的“专家角色”)**

在您的“AI 专家剧组”中，`codex.invoke` 扮演着独一无二的 **“情境分析师”** 兼 **“规则书记官”**。

想象一下您要为“首席作家”（LLM）准备一份关于“甘道夫”这个角色的系统提示词。在传统模式下，您可能会将甘道夫的所有设定——“他是个巫师”、“他拿着法杖”、“他很睿智”、“他此刻很疲惫”——都写在一起。

但 `codex.invoke` 认为这是一种浪费。“他是个巫师”是**不常变的基础设定**，而“他此刻很疲惫”是**临时的情境状态**。这位“情境分析师”的专长，就是将这两者分层管理，并根据需要进行智能组合：

1.  **分层知识**: 它会从 `lore.codices`（您的“世界法典”）中读取那些稳定、长期的基础知识（如角色背景、世界规则）。
2.  **情境注入**: 同时，它也会查看 `moment.codices`（您的“即时快照”），寻找那些临时的、与当前场景紧密相关的知识（如“角色正处于中毒状态”、“场景中刚下过雨”）。`moment` 中的知识甚至可以**在逻辑上暂时覆盖** `lore` 中的基础设定（例如，将“他身穿灰色斗篷”临时覆盖为“他的斗篷沾满泥土”）。
3.  **条件激活**: 它不会把所有知识都堆砌在一起，而是会根据您提供的“激活源文本”，只挑选出那些与当前话题最相关的知识条目。
4.  **优先级排序**: 最后，它会将所有被激活的知识，按照您设定的优先级（重要程度）从高到低排列组合，生成一份逻辑清晰、层次分明、完全为当前情境量身定制的“创作指南”。

这份动态生成的“指南”，确保了“首席作家”收到的永远是最新、最 relevant 的信息，从而使其创作能够精准地反映出世界的动态变化。

#### **配置参数 (Configuration)**

*   **`from`** (`列表`, **必需**)
    *   一个列表，定义了本次知识组合的“扫描任务”。列表中的每一项都是一个独立的对象，告诉“分析师”要去哪个知识库里、用什么线索来寻找知识。每个扫描任务对象包含：
        *   `codex` (`文本`): 您要扫描的知识库的名称（例如 `"character_background_gandalf"` 或 `"world_rules"`）。
        *   `source` (`文本`, 可选): “激活源文本”。这是一个包含宏的字符串，其最终的文本结果，将作为触发该知识库中“关键词激活”模式条目的“线索”。如果留空，则只会激活该知识库中那些“总是开启”的条目。

*   **`recursion_enabled`** (`布尔值`, 可选)
    *   是否启用“递归激活”，默认为 `false`。
    *   这是一个非常强大的高级功能。如果启用，一个被激活的知识条目，在它被渲染成文本后，其**自身的文本内容**可以作为**新的“线索”**，去**再次触发**知识库中其他尚未被激活的条目。这能让知识之间形成逻辑链，构建出极其富有深度和细节的最终文本。

*   **`debug`** (`布尔值`, 可选)
    *   是否在输出中包含详细的调试信息，默认为 `false`。

#### **输出 (Output)**

执行成功后，它会向节点的“工作台” (`pipe`) 输出：

*   `output` (`文本`): 所有被激活的知识条目，在经过优先级排序后，拼接成的最终文本。
*   `debug_info` (`对象`, 仅在 `debug` 为 `true` 时出现): 包含本次执行详细信息的对象，例如哪些条目被成功激活。

#### **协同工作 (Synergy in Practice): 动态生成 NPC 描述**

**目标**: 我们要创建一个“NPC 描述生成器”节点。它能根据玩家当前的输入，结合 NPC 自身固有的背景和临时的状态，动态生成一段丰富、立体的描述文本，然后交给 LLM 去润色。

**您的知识库设计 (`lore` 和 `moment` 中)**:

*   **在 `lore.codices` (世界法典) 中，您定义了甘道夫的基础知识库 `npc_gandalf`**:
    *   条目1 (id: `base_identity`, priority: 100): "甘道夫是一位埃斯塔力，是派来中土帮助对抗索伦的迈雅。" (总是开启)
    *   条目2 (id: `appearance`, priority: 90): "他通常以一个戴着尖顶蓝帽、身穿灰色斗篷、手持法杖的老人形象出现。" (总是开启)
    *   条目3 (id: `on_balrog`, priority: 120): "当提及炎魔时，他的眼神会变得无比凝重，因为他曾在摩瑞亚与之同归于尽。" (关键词激活, keywords: `["炎魔", "巴洛格"]`)

*   **在 `moment.codices` (即时快照) 中，您注入了甘道夫的当前状态**:
    *   条目1 (id: `current_action`, priority: 110): "他此刻看起来筋疲力尽，正低声吟唱着咒语，法杖顶端发出微光。" (总是开启)

**您的节点 (`describe_gandalf`) 配置**:

1.  在它的“行动清单” (`run`) 中，添加一条指令，装备 `codex.invoke` 技能。
2.  **配置参数**:
    *   **`from`**: 您会在这里添加一个“扫描任务”：
        *   `codex`: `"npc_gandalf"` (告诉它去扫描甘道夫的知识库)
        *   `source`: `{{ run.triggering_input.user_query }}` (告诉它用玩家的当前输入作为“线索”)

**三种不同的执行结果**:

*   **场景一：玩家输入为 “我眼前这个人是谁？”**
    *   `source` 文本不包含任何关键词。
    *   `codex.invoke` 会激活所有“总是开启”的条目，并按优先级组合。
    *   **最终输出**:
        ```
        他此刻看起来筋疲力尽，正低声吟唱着咒语，法杖顶端发出微光。

        甘道夫是一位埃斯塔力，是派来中土帮助对抗索伦的迈雅。

        他通常以一个戴着尖顶蓝帽、身穿灰色斗篷、手持法杖的老人形象出现。
        ```

*   **场景二：玩家输入为 “我想知道更多关于炎魔的事情。”**
    *   `source` 文本命中了关键词“炎魔”。
    *   `codex.invoke` 不仅会激活所有“总是开启”的条目，还会激活那条关于炎魔的、**优先级最高**的知识。
    *   **最终输出**:
        ```
        当提及炎魔时，他的眼神会变得无比凝重，因为他曾在摩瑞亚与之同归于尽。

        他此刻看起来筋疲力尽，正低声吟唱着咒语，法杖顶端发出微光。

        甘道夫是一位埃斯塔力，是派来中土帮助对抗索伦的迈雅。

        他通常以一个戴着尖顶蓝帽、身穿灰色斗篷、手持法杖的老人形象出现。
        ```

这个例子完美地诠释了 `codex.invoke` 的核心价值：它不是一个简单的文本拼接工具，而是一个能够**感知上下文、区分知识层级、并根据优先级进行智能裁决**的“世界规则引擎”。通过它，您可以构建出一个真正对情境变化做出细腻反应的、“活”着的 AI 世界。


### **第五章：`system.flow.call` — 艺术的委派**

#### **一句话摘要**

该运行时允许您从当前的工作流中，调用并执行另一个独立的、可复用的图（我们称之为“子图”），就像在电影拍摄中，主导演将一个特定的场景全权委托给一个专业的“第二摄制组”。

#### **为何存在？(它的“专家角色”)**

在您的“AI 专家剧组”中，`system.flow.call` 扮演着 **“副导演”** 或 **“场景调度员”** 的关键角色。

随着您创造的世界日益宏大，您会发现某些流程需要被反复执行。例如，“角色受到伤害”的逻辑——无论伤害是来自刀剑、魔法还是陷阱，其核心处理流程（扣减生命值、检查是否死亡、记录战斗日志）都是相同的。

在没有“副导演”的情况下，您将不得不在您的主创作蓝图 (`main` 图) 中，为每一种受伤情景都重复地搭建一套相同的节点。这不仅极其繁琐，而且一旦您想修改伤害逻辑（比如增加一个“受伤音效”），您就必须找到所有地方逐一修改，这无疑是一场噩梦。

“副导演” (`system.flow.call`) 的诞生，正是为了解决这一根本性的“重复劳动”问题。它为您带来了两大革命性的优势：

1.  **模块化与复用 (创建“标准作业流程”)**:
    您可以将一套通用的逻辑（如“伤害处理”、“好感度计算”、“物品鉴定”）封装在一个独立的子图中。`system.flow.call` 则像一个命令，随时可以调用这个“标准作业流程”。您只需定义一次，便可在成百上千个不同地方复用它。

2.  **清晰与组织 (保持主蓝图的整洁)**:
    它能让您的主蓝图 (`main` 图) 变得极其干净和高层。主蓝图不再需要关心“生命值具体怎么扣减”的细节，它只需下达一个清晰的指令：“*执行‘伤害处理’流程*”。所有复杂的实现细节都被优雅地隐藏在子图中。这使得您作为“总设计师”，可以始终聚焦于故事的宏大走向，而不是被繁杂的底层实现所淹没。

#### **配置参数 (Configuration)**

当您为节点装备上 `system.flow.call` 这个“委派”技能时，您需要配置以下参数来指导它的工作：

*   **`graph`** (`文本`, **必需**)
    *   **您要委派给哪个“专业团队”（子图）？** 在这里，您需要填写那个您希望调用的、可复用的子图的名称。例如，`"process_damage_logic"` 或 `"character_relationship_update"`。

*   **`using`** (`对象/字典`, 可选)
    *   **您要给这个专业团队下达什么“具体指令”和“道具”？** 这是 `system.flow.call` 最强大、最核心的参数。它允许您将当前图中的数据，动态地传递给子图使用。
    *   您可以把 `using` 想象成是您递交给“第二摄制组”的**“拍摄通告单”**。
    *   在这个“通告单”中，您可以定义任意多个条目。每一条目的“**名称**”（Key）将成为子图内部一个**临时的、可被引用的“数据源”**（表现为一个特殊的节点）。而该条目的“**值**”（Value）——这里**完美支持宏**——就是您要传递给这个“数据源”的具体数据。
    *   例如，您在 `using` 中配置了：`{ "damage_amount": {{ run.triggering_input.sword_damage }} }`。
    *   **幕后发生的事**: 当子图开始执行时，它内部会自动获得一个名为 `damage_amount` 的特殊“输入节点”。这个节点已经完成了它的工作，并且其输出 (`output`) 就是您传递进来的那个 `sword_damage` 的值。子图内的其他节点，就可以通过宏 `{{ nodes.damage_amount.output }}` 来轻松使用这个数据了。

#### **输出 (Output)**

当“副导演”委派的任务完成后（即子图执行完毕），“专业团队”会交回一份**完整的“工作报告”**。这份报告会被放到当前节点的“工作台” (`pipe`) 上。

*   `output` (`对象`): 这是一个包含了子图**所有**最终节点状态的对象。
    *   对象的“**键**”（Key）是子图内部节点的 **ID**。
    *   对象的“**值**”（Value）是该节点对应的**完整输出**。

这意味着，在主图中，您可以通过 `{{ pipe.output.子图中的节点ID.output }}` 这样的宏，来精确地获取子图内部任何一个节点的最终成果，从而实现主图与子图之间的双向数据交流。

#### **协同工作 (Synergy in Practice): 构建可复用的“伤害处理模块”**

**目标**: 在一个角色扮演游戏中，玩家可能因多种原因受伤。我们将创建一个可复用的子图来统一处理所有伤害事件，避免重复劳动。

**第一步：设计“专业团队”的工作流程 (创建子图 `process_damage`)**

我们在编辑器中创建一个名为 `process_damage` 的新图。这个图就是我们的“伤害处理专家组”。

*   **子图的输入**: 这个团队需要知道“本次要处理多少点伤害”。我们通过 `using` 参数来接收这个信息。我们约定，传递进来的数据源名为 `damage_input`。

*   **子图的内部流程**:
    1.  **节点 A: `apply_damage` (执行伤害)**
        *   **技能**: `system.execute` (一个可以执行代码的技能)
        *   **任务**: `{{ moment.player_hp -= nodes.damage_input.output }}`
            *   *(这句宏的意思是：找到名为 `damage_input` 的那个“数据源”节点，用它的输出值来扣减玩家的生命值。)*
    2.  **节点 B: `check_for_death` (检查存活)**
        *   **技能**: `system.execute`
        *   **任务**: `{{ 'DEFEATED' if moment.player_hp <= 0 else 'ALIVE' }}`
        *   **输出**: 这个节点会输出一个字符串 `"DEFEATED"` 或 `"ALIVE"`，作为本次处理的最终状态。

---

**第二步：在主蓝图中委派任务 (在 `main` 图中调用子图)**

现在，在我们的主故事线 (`main` 图) 中，发生了两个不同的事件：

**事件一：玩家被哥布林击中**

1.  创建一个节点 `goblin_attack`。
2.  为它装备 `system.flow.call` 技能。
3.  **配置参数**:
    *   **`graph`**: `"process_damage"` (委派给我们的伤害处理专家组)
    *   **`using`**: `{ "damage_input": 15 }` (通告单：明确告知专家组，本次伤害值为 15)

**事件二：玩家踩到了火焰陷阱**

1.  在图的另一处，创建一个节点 `trap_triggered`。
2.  同样为它装备 `system.flow.call` 技能。
3.  **配置参数**:
    *   **`graph`**: `"process_damage"` (再次委派给同一个专家组)
    *   **`using`**: `{ "damage_input": {{ lore.traps.fire_trap.base_damage }} }` (通告单：本次伤害值从世界法典的陷阱定义中动态获取)

---

**革命性的成果**

1.  **极致的简洁**: 伤害处理的复杂逻辑（扣血、检查死亡）被完美地封装在了 `process_damage` 子图中。我们的主蓝图 (`main` 图) 变得异常清晰，只剩下“哥布林攻击”和“陷阱触发”这样高层级的事件描述。
2.  **轻松的维护**: 某天，您觉得伤害逻辑需要升级——比如，在检查死亡后，还需要播放一段“游戏结束”的过场动画。您**无需**修改主图中的任何内容。您只需打开 `process_damage` 这个子图，在 `check_for_death` 节点后添加一个新的“播放动画”节点即可。这一处修改，会自动应用到游戏中所有发生伤害的地方。
3.  **清晰的数据流**: 在 `goblin_attack` 节点执行完毕后，您可以通过 `{{ pipe.output.check_for_death.output }}` 立即获知玩家是否在此次攻击中被击败，从而决定主故事线接下来的走向。

通过 `system.flow.call` 这位得力的“副导演”，您作为“总设计师”的工作被提升到了一个全新的战略层面。您不再是疲于奔命的工兵，而是一位真正能够运筹帷幄、搭建和指挥模块化、可扩展、可维护的动态世界的架构师。


### **第六章：`system.flow.map` — 并行的艺术**

#### **一句话摘要**

该运行时遍历您提供的一个列表，并为列表中的**每一项**，都**并行地、独立地**启动并执行一次指定的子图，最终将所有执行结果汇总起来。

#### **为何存在？(它的“专家角色”)**

在您的“AI 专家剧组”中，`system.flow.map` 扮演着 **“并行计算协调员”** 或 **“多线程任务主控”** 的非凡角色。

想象以下几个在动态世界中极其常见的场景：

*   **场景一 (战斗)**: 战场上有五名敌人，您需要为**每一名**敌人独立计算它的下一步行动意图。
*   **场景二 (叙事)**: 您需要遍历玩家物品栏里的**每一件**魔法物品，并为它们分别生成一段独特的鉴定描述。
*   **场景三 (世界模拟)**: 在一个回合结束时，您需要更新地图上**每一个** NPC 的状态，让他们各自根据自己的情况，寻找食物、休息或移动。

在传统的线性流程中，您只能一个接一个地处理这些任务：先计算第一个敌人的意图，等待完成后，再计算第二个……如果每个任务都需要调用一次 LLM，那么五个敌人就意味着五倍的等待时间。这在需要快速响应的交互式应用中是完全不可接受的。

`system.flow.map` 的诞生，就是为了**粉碎这种线性的桎梏**。它引入了“**并行处理**”的革命性概念。

它的核心工作模式是“**分而治之**”：

1.  **接收一个“目标清单” (一个列表)**: 例如，一个包含所有敌人对象的列表。
2.  **指定一个“处理流程” (一个子图)**: 您需要告诉它，对于清单上的每一个目标，应该执行哪一套标准作业流程（例如，我们上一章创建的 `process_damage` 子图）。
3.  **瞬间分派 (启动并行任务)**: “协调员”会像拥有“影分身之术”一样，为列表中的**每一项**都瞬间创建一个独立的“克隆团队”，并让它们**同时开始**执行指定的子图流程。五个敌人，就意味着五个“意图计算小组”在同一时间并行工作。
4.  **等待并汇总 (收集所有结果)**: “协调员”会耐心等待所有“克隆团队”完成它们各自的任务，然后将所有团队的工作报告（输出结果）收集起来，整理成一个最终的列表，一次性交付给您。

通过这种方式，处理五个敌人的总时间，将不再是五者之和，而仅仅取决于**耗时最长的那一个**。这为构建复杂、宏大且能实时响应的 AI 世界，提供了至关重要的性能基石。

#### **配置参数 (Configuration)**

当您为节点装备上 `system.flow.map` 这个“并行协调”神技时，它的配置比 `system.flow.call` 更为精妙和强大：

*   **`list`** (`列表`, **必需**)
    *   **您要处理的“目标清单”是什么？** 在这里，您需要提供一个列表。通常使用宏来动态获取，例如 `{{ moment.enemies_in_area }}` 或 `{{ nodes.get_inventory.output }}`。

*   **`graph`** (`文本`, **必需**)
    *   **为每个目标应用的“标准作业流程”是哪个子图？** 在这里，您需要填写那个可复用的子图的名称，例如 `"calculate_enemy_intent"`。

*   **`using`** (`对象/字典`, 可选)
    *   **您要给每一个“克隆团队”下达什么“具体指令”和“道具”？** 这与 `system.flow.call` 中的 `using` 非常相似，但有一个至关重要的区别：在这里，您可以使用一个**特殊的、名为 `source` 的上下文对象**。
    *   **`source` 对象**: 当 `system.flow.map` 为列表中的某一项启动子图时，它会自动创建一个名为 `source` 的临时上下文。这个 `source` 对象包含了关于**当前正在处理的那一项**的所有信息：
        *   **`source.item`**: 代表列表中的**当前项本身**。如果您的 `list` 是一个敌人对象列表，那么 `source.item` 就是那个敌人对象。
        *   **`source.index`**: 代表当前项在原始列表中的**索引位置**（从 0 开始）。
    *   **示例**: 您可以在 `using` 中这样配置：
        ```
        { 
          "enemy_data": "{{ source.item }}",
          "battlefield_context": "{{ moment.battlefield }}"
        }
        ```
        *   **幕后发生的事**: 当处理第一个敌人时，传递给第一个子图的 `enemy_data` 将是 `list[0]`；处理第二个敌人时，传递给第二个子图的 `enemy_data` 将是 `list[1]`，依此类推。而 `battlefield_context` 对所有子图来说都是相同的。这实现了“**通用上下文**”与“**个性化数据**”的完美结合。

*   **`collect`** (`任意类型`, 可选)
    *   **当所有团队都交回“工作报告”后，您需要从中提取什么核心信息？** 这是一个极其强大的“**数据聚合**”工具。
    *   如果不提供 `collect`，`system.flow.map` 的最终输出将是一个列表，其中每一项都是一次子图执行的**完整工作报告**（即子图所有最终节点的完整输出对象）。
    *   但很多时候，我们只关心每个子图执行结果中的某个特定部分。`collect` 允许您提供一个**宏**，这个宏会在**每一次**子图执行结束后，在其**独立的上下文**中运行一次。
    *   **示例**: 假设您的 `calculate_enemy_intent` 子图，最终会有一个名为 `final_decision` 的节点，其输出是 `"ATTACK_PLAYER"`。如果您只想收集所有敌人的最终决策，您可以这样配置 `collect`:
        ```
        "{{ nodes.final_decision.output }}"
        ```
    *   **幕后发生的事**:
        1.  第一个子图执行完毕，`collect` 宏会在它的结果上运行，提取出 `"ATTACK_PLAYER"`。
        2.  第二个子图执行完毕，`collect` 宏又会在它的结果上运行，可能提取出 `"CAST_HEAL"`。
        3.  ...
        4.  最终，`system.flow.map` 的输出将不再是庞大的完整报告列表，而是一个**极其干净、只包含核心信息的列表**: `["ATTACK_PLAYER", "CAST_HEAL", ...]`。

#### **输出 (Output)**

*   **不使用 `collect` 时**: `output` (`列表`): 一个列表，其中每一项都是一次子图执行的**完整**最终节点状态对象。
*   **使用 `collect` 时**: `output` (`列表`): 一个列表，其中每一项都是 `collect` 宏从对应子图执行结果中**提取**出的值。

#### **协同工作 (Synergy in Practice): 战场 AI 的并行决策**

**目标**: 战场上有三只不同类型的哥布林。我们需要为它们并行地、独立地生成各自的行动计划，然后汇总所有计划。

**第一步：设计“单兵作战AI”的工作流程 (创建子图 `goblin_ai_logic`)**

这个子图负责处理**单个**哥布林的决策。

*   **子图的输入**: 我们约定，它需要接收两个数据：`goblin` (哥布林自己的数据) 和 `player_status` (玩家的当前状态)。

*   **子图的内部流程**:
    1.  **节点 A: `decide_action` (决策)**
        *   **技能**: `llm.default`
        *   **任务 (prompt)**:
            ```
            你是一个哥布林。你的数据是：{{ nodes.goblin.output }}。
            玩家的状态是：{{ nodes.player_status.output }}。
            根据你的类型（战士或萨满），决定你的下一步行动是“攻击”还是“治疗同伴”，只返回一个词。
            ```
    2.  **节点 B: `format_log_entry` (格式化日志)**
        *   **技能**: `system.data.format`
        *   **任务**: 将决策格式化为一条战斗日志。
        *   **`template`**: `"哥布林 {nodes.goblin.output.name} 决定: {nodes.decide_action.output}!"`

---

**第二步：在主蓝图中指挥战场 (在 `main` 图中调用 `system.flow.map`)**

在主图的战斗流程中，我们创建一个节点 `coordinate_enemy_turn`。

1.  为它装备 `system.flow.map` 技能。
2.  **配置参数**:
    *   **`list`**: `{{ moment.enemies_on_field }}`
        *   *(假设这是一个列表，包含了战场上所有哥布林的对象)*
    *   **`graph`**: `"goblin_ai_logic"`
        *   *(委派给我们的“单兵作战AI”流程)*
    *   **`using`**:
        ```
        {
          "goblin": "{{ source.item }}",
          "player_status": "{{ moment.player }}"
        }
        ```
        *   *(通告单：为每个“克隆团队”，都将列表中的当前哥布林 `source.item` 作为 `goblin` 数据传递，并将统一的 `moment.player` 作为 `player_status` 数据传递)*
    *   **`collect`**: `{{ nodes.format_log_entry.output }}`
        *   *(聚合指令：我们不关心每个子图内部的复杂计算过程，我们只需要收集最终格式化好的那条战斗日志)*

---

**革命性的成果**

当 `coordinate_enemy_turn` 节点运行时，Hevno 引擎会瞬间为战场上的三只哥布林，**同时启动三个** `goblin_ai_logic` 子图实例。它们会并行地与 LLM 通信，进行决策。

假设三只哥布林分别决定了“攻击”、“攻击”和“治疗同伴”。

由于我们使用了 `collect` 参数，这个节点的最终输出 (`{{ pipe.output }}`) 将会是一个非常干净、可直接使用的**字符串列表**:

```
[
  "哥布林 Grug 决定: 攻击!",
  "哥布林 Zog 决定: 攻击!",
  "哥布林 Nastyz 决定: 治疗同伴!"
]
```

您不仅以**接近处理单个敌人**的时间，完成了对所有敌人的决策，还得到了一份可以直接用于生成战斗旁白的、完美聚合的数据。

`system.flow.map` 是 HevkeyCode 引擎从“线性脚本”迈向“高性能并行计算平台”的标志。它将赋予您作为“世界总设计师”前所未有的能力，去构建那些真正宏大、复杂、且拥有众多独立活动角色的动态世界，而无需成为一名并发编程专家。